# ERROR ANALYSIS
## 360-420-DW - 0001
## Yu Shi Wang & Yu Qi Zhou

## Distributions of Model Accuracy
1. Each time we run the model, our functions shuffles the data set available. After shuffling, all the numbers are in a completely different order than the previous run. Therefore, in every run, when we separate training set and test set after this step, there is a different test set and a different training set. After training with a different dataset, for some test points, the nearest neighbors might provide different answers, thus, the shuffling of the data and thus the randomness of the data in the training set and in the test set contributes to this variance in accuracy. 

2. We did it ðŸ˜Š

3. On average, our modelâ€™s **accuracy** is **99.633%**, with a **standard deviation** of **2.01E-24**. We can compare our modelâ€™s performance to the basic frequency that each label appears. For example, in this case, the label benign appears at a frequency of 65.2 % and the label malignant appears at a frequency of 34.8%. Therefore, our algorithm should at least have an accuracy of 65.2%. 



## Analysis of different error types
1. A **false positive** is an error that occurs when the classifier indicates the presence of something when the latter is actually absent. For example, in our dataset, if someone doesnâ€™t have malignant cancer but the classifier indicates that the person has malignant cancer, it is a false positive. 

2. A **false negative** is an error that occurs when the classifier indicates the absence of something when the latter is actually present. Once again, for our dataset, for example, if the classifier indicates that the person doesnâ€™t have malignant cancer but the person actually has malignant cancer, it is a false negative. 

3. **Precision** refers to the accuracy of the elements selected by the classifier. Precision evaluates the number of rightly classified elements versus the number of elements that the classifier has identified as being positive. To calculate the precision, we divide the number of true positives by the sum of true positives and false positives. In our case, precision would refer to, among the people who have been classified as having malignant cancer, how many actually have malignant cancer. To calculate this precision, we would divide the number of people having malignant cancer and having been classified as so by the sum of people that have been classified as having malignant cancer.  If the precision is 100%, in our case, that means that every person identified as havig malignant cancer has been identified as having malignant cancer without any false positives. However, to obtain a precision of 100%, it would mean that the test set is exactly the same as our data set. Thus, it would also mean that the recall is 100%. However, it would also mean that we have to change our test set and we did not test the efficiency of our model.  

4. On the other hand, **recall** refers to the number of relevant items that the classifier has selected. Recall evaluates the number of items right classified rightly in a certain category versus the number of items that are supposed to be in that category. For example, in our case, recall evaluates how many people having malignant cancer have been classified as having malignant cancer. To evaluate recall, we divide the number of true positives by the sum of false negatives and true positives. In our case, we would divide the number of people having malignant cancer and having identified as having malignant cancer divided by the sum of the people having malignant cancer and having been identified as having malignant cancer and the people having malignant cancer but having been identified as not having malignant cancer. For the recall to become 100%, the easiest way is to set everyone as having malignant cancer. Thus, every single person having malignant cancer will surely be identified as having malignant cancer However, in this case, the precision will suffer as, all the people that have benign cancer will be identified as having malignant cancer, thus creating a big number of false positives and diminishing greatly the precision.

5. The hyperparameter k represents the number of nearest neighbours that the algorithm will take to evaluate the class in which the item belongs in. The class with the highest number of nearest neighbours in the group of nearest neighbours taken with the parameter k will be the class in which the item will be classed into. The higher the number of k, the more datapoints we will take to evaluate. In this case, the accuracy suffers as the error will be higher as more data points will be identified. In this case, true positives will become a higher number, thus the recall will be higher. However, at the same time the more points we take, the precision will suffer as more items might be identified as positive and thus, there is a higher probability of having a false positive. On the other hand, if the hyperparameter k is too small (i.e. 1), items that are in an ambiguous region might (i.e. items whose 2 closest neighbors are from different classes and are almost equidistant from the item we are trying to class) have a difficulty getting classified accurately into one class or another. In the case of less points taken in the case of a smaller k, there would be less true positives identified, thus maybe reducing the recall. On the other hand, as there are less points taken, it would also reduce the number of false positives produced, as, overall, the criterias for producing a positive are stricter, thus augmenting the precision of our model. 


